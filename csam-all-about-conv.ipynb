{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":196452,"sourceType":"datasetVersion","datasetId":84954},{"sourceId":4149928,"sourceType":"datasetVersion","datasetId":2450619}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\nfrom tensorflow import keras\nfrom keras import Sequential,layers\nfrom keras.layers import Dense,Conv2D,MaxPooling2D,Flatten,BatchNormalization,Dropout,AveragePooling2D,Softmax\nfrom tensorflow.keras.layers import ReLU,GlobalAveragePooling3D\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.optimizers import SGD,Adam\nfrom tensorflow.keras.losses import BinaryCrossentropy\nimport numpy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-27T12:10:48.737756Z","iopub.execute_input":"2024-06-27T12:10:48.738535Z","iopub.status.idle":"2024-06-27T12:11:01.435078Z","shell.execute_reply.started":"2024-06-27T12:10:48.738504Z","shell.execute_reply":"2024-06-27T12:11:01.434192Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-06-27 12:10:50.631869: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-27 12:10:50.632013: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-27 12:10:50.779420: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"train_dir = '/kaggle/input/dogs-vs-cats/train'\ntest_dir = '/kaggle/input/dogs-vs-cats/test'","metadata":{"execution":{"iopub.status.busy":"2024-06-27T12:11:01.436470Z","iopub.execute_input":"2024-06-27T12:11:01.437062Z","iopub.status.idle":"2024-06-27T12:11:01.441670Z","shell.execute_reply.started":"2024-06-27T12:11:01.437033Z","shell.execute_reply":"2024-06-27T12:11:01.440790Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train_dataset = image_dataset_from_directory(\n    train_dir,\n    image_size=(224, 224),\n    batch_size=32,\n    label_mode='int',\n    shuffle=True\n)\ntest_dataset = image_dataset_from_directory(\n    test_dir,\n    image_size=(224, 224),\n    batch_size=32,\n    label_mode='int',\n    shuffle=False\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-27T12:12:44.936383Z","iopub.execute_input":"2024-06-27T12:12:44.937193Z","iopub.status.idle":"2024-06-27T12:13:25.301911Z","shell.execute_reply.started":"2024-06-27T12:12:44.937161Z","shell.execute_reply":"2024-06-27T12:13:25.300944Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Found 20000 files belonging to 2 classes.\nFound 5000 files belonging to 2 classes.\n","output_type":"stream"}]},{"cell_type":"code","source":"def preprocess(image, label):\n    image = tf.cast(image, tf.float32) / 255.0\n    return image, label\ntrain_dataset = train_dataset.map(preprocess)\ntest_dataset = test_dataset.map(preprocess)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-27T12:19:30.823897Z","iopub.execute_input":"2024-06-27T12:19:30.824651Z","iopub.status.idle":"2024-06-27T12:19:30.885906Z","shell.execute_reply.started":"2024-06-27T12:19:30.824621Z","shell.execute_reply":"2024-06-27T12:19:30.884934Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_images = []\ntrain_labels = []\nfor images, labels in train_dataset:\n    train_images.append(images)\n    train_labels.append(labels)\n\ntest_images = []\ntest_labels = []\nfor images, labels in test_dataset:\n    test_images.append(images)\n    test_labels.append(labels)\n\n# Convert to tensors\ntrain_images = tf.concat(train_images, axis=0)\ntrain_labels = tf.concat(train_labels, axis=0)\ntest_images = tf.concat(test_images, axis=0)\ntest_labels = tf.concat(test_labels, axis=0)\n\n# Check shapes\nprint(f\"Train images shape: {train_images.shape}\")\nprint(f\"Train labels shape: {train_labels.shape}\")\nprint(f\"Test images shape: {test_images.shape}\")\nprint(f\"Test labels shape: {test_labels.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-27T10:20:10.810286Z","iopub.execute_input":"2024-06-27T10:20:10.810643Z","iopub.status.idle":"2024-06-27T10:21:17.967969Z","shell.execute_reply.started":"2024-06-27T10:20:10.810613Z","shell.execute_reply":"2024-06-27T10:21:17.966835Z"},"trusted":true},"execution_count":5,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     test_labels\u001b[38;5;241m.\u001b[39mappend(labels)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Convert to tensors\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m train_images \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m train_labels \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconcat(train_labels, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     16\u001b[0m test_images \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconcat(test_images, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:5883\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5881\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   5882\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 5883\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","\u001b[0;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__ConcatV2_N_625_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[20000,224,224,3] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:ConcatV2] name: concat"],"ename":"ResourceExhaustedError","evalue":"{{function_node __wrapped__ConcatV2_N_625_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[20000,224,224,3] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:ConcatV2] name: concat","output_type":"error"}]},{"cell_type":"code","source":"class inception(keras.Model):\n    def __init__(self,filter_1,filter_3,filter_5):\n        super(inception,self).__init__()\n        self.conv1=Conv2D(filter_1,kernel_size=(1,1),strides=1,padding='same',activation='relu')\n        self.seq3=Sequential([Conv2D(filter_1,kernel_size=(1,1),strides=1,padding='same',activation='relu'),\n                              Conv2D(filter_3,kernel_size=(3,3),strides=1,padding='same',activation='relu')\n                              ])\n        self.seq5=Sequential([Conv2D(filter_1,kernel_size=(1,1),strides=1,padding='same',activation='relu'),\n                              Conv2D(filter_5,kernel_size=(5,5),strides=1,padding='same',activation='relu')\n                             ])\n        self.seq=Sequential([MaxPooling2D(pool_size=(3,3),strides=1,padding='same'),\n                             Conv2D(filter_1,kernel_size=(1,1),strides=1,padding='same',activation='relu')\n                             ])\n    def call(self,inputs):\n        x1=self.conv1(inputs)\n        x2=self.seq3(inputs)\n        x3=self.seq5(inputs)\n        x4=self.seq(inputs)\n        return tf.concat([x1,x2,x3,x4],axis=-1)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-27T12:19:36.930022Z","iopub.execute_input":"2024-06-27T12:19:36.930389Z","iopub.status.idle":"2024-06-27T12:19:36.940337Z","shell.execute_reply.started":"2024-06-27T12:19:36.930360Z","shell.execute_reply":"2024-06-27T12:19:36.939473Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class Network(keras.Model):\n  def __init__(self):\n    super(Network,self).__init__()\n    self.seq1=Sequential([Conv2D(64,kernel_size=(7,7),strides=2,padding='same',activation='relu'),\n                          MaxPooling2D(pool_size=(3,3),strides=2,padding='valid'),\n                          BatchNormalization(),\n                          Conv2D(64,kernel_size=(1,1),strides=1,padding='valid',activation='relu'),\n                          Conv2D(192,kernel_size=(3,3),strides=1,padding='same',activation='relu'),\n                          BatchNormalization(),\n                          MaxPooling2D(pool_size=(3,3),strides=2,padding='same'),\n                          inception(64,96,32),\n                          inception(128,128,96),\n                          MaxPooling2D(pool_size=(3,3),strides=2,padding='same'),\n                          inception(192,96,32)\n                               ])\n    self.classify1=Sequential([AveragePooling2D(pool_size=(5,5),strides=3,padding='valid'),\n                               Conv2D(160,kernel_size=(1,1),strides=1,padding='same',activation='relu'),\n                               Flatten(),\n                               Dense(1024,activation='relu'),\n                               Dropout(0.7),\n                               Dense(512,activation='relu'),\n                               Dropout(0.7),\n                               Dense(1,activation='sigmoid')\n                               ])\n    self.seq2=Sequential([inception(160,112,80),\n                          inception(128,128,128),\n                          inception(112,144,160)\n                              ])\n    self.classify2=Sequential([AveragePooling2D(pool_size=(5,5),strides=3,padding='valid'),\n                               Conv2D(112,kernel_size=(1,1),strides=1,padding='same',activation='relu'),\n                               Flatten(),\n                               Dense(1024,activation='relu'),\n                               Dropout(0.7),\n                               Dense(512,activation='relu'),\n                               Dropout(0.7),\n                               Dense(1,activation='sigmoid')\n                               ])\n    self.seq3=Sequential([inception(256,160,160),\n                          MaxPooling2D(pool_size=(3,3),strides=2,padding='same'),\n                          inception(256,160,160),\n                          inception(384,192,64),\n                          AveragePooling2D(pool_size=(7,7),strides=1,padding='valid'),\n                          Flatten(),\n                          Dropout(0.4),\n                          Dense(512,activation='relu'),\n                          Dropout(0.4),\n                          Dense(256,activation='relu'),\n                          Dropout(0.4),\n                          Dense(1,activation='sigmoid')\n                          ])\n\n  def call(self,inputs):\n    x=self.seq1(inputs)\n    classifier1=tf.squeeze(self.classify1(x))\n    x=self.seq2(x)\n    classifier2=tf.squeeze(self.classify2(x))\n    final_classifier=tf.squeeze(self.seq3(x))\n\n    \n    return classifier1,classifier2,final_classifier","metadata":{"execution":{"iopub.status.busy":"2024-06-27T12:19:39.839120Z","iopub.execute_input":"2024-06-27T12:19:39.840054Z","iopub.status.idle":"2024-06-27T12:19:39.856172Z","shell.execute_reply.started":"2024-06-27T12:19:39.840020Z","shell.execute_reply":"2024-06-27T12:19:39.855118Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class Inception(Model):\n    def __init__(self, filter_1, filter_3, filter_5):\n        super(Inception, self).__init__()\n        self.conv1 = Conv2D(filter_1, kernel_size=(1, 1), strides=1, padding='same', activation='relu')\n        self.seq3 = Sequential([\n            Conv2D(filter_1, kernel_size=(1, 1), strides=1, padding='same', activation='relu'),\n            Conv2D(filter_3, kernel_size=(3, 3), strides=1, padding='same', activation='relu')\n        ])\n        self.seq5 = Sequential([\n            Conv2D(filter_1, kernel_size=(1, 1), strides=1, padding='same', activation='relu'),\n            Conv2D(filter_5, kernel_size=(5, 5), strides=1, padding='same', activation='relu')\n        ])\n        self.seq = Sequential([\n            MaxPooling2D(pool_size=(3, 3), strides=1, padding='same'),\n            Conv2D(filter_1, kernel_size=(1, 1), strides=1, padding='same', activation='relu')\n        ])\n\n    def call(self, inputs):\n        x1 = self.conv1(inputs)\n        x2 = self.seq3(inputs)\n        x3 = self.seq5(inputs)\n        x4 = self.seq(inputs)\n        return tf.concat([x1, x2, x3, x4], axis=-1)\n\n\nclass TransformerBlock(Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n        super(TransformerBlock, self).__init__()\n        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = Sequential([\n            Dense(ff_dim, activation='relu'),\n            Dense(embed_dim)\n        ])\n        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n        self.dropout1 = Dropout(rate)\n        self.dropout2 = Dropout(rate)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)\n\n\nclass Network(Model):\n    def __init__(self):\n        super(Network, self).__init__()\n        self.seq1 = Sequential([\n            Conv2D(64, kernel_size=(7, 7), strides=2, padding='same', activation='relu'),\n            MaxPooling2D(pool_size=(3, 3), strides=2, padding='valid'),\n            BatchNormalization(),\n            Conv2D(64, kernel_size=(1, 1), strides=1, padding='valid', activation='relu'),\n            Conv2D(192, kernel_size=(3, 3), strides=1, padding='same', activation='relu'),\n            BatchNormalization(),\n            MaxPooling2D(pool_size=(3, 3), strides=2, padding='same'),\n            Inception(64, 96, 32),\n            Inception(128, 128, 96),\n            MaxPooling2D(pool_size=(3, 3), strides=2, padding='same'),\n            Inception(192, 96, 32)\n        ])\n        \n        self.transformer = TransformerBlock(embed_dim=384, num_heads=4, ff_dim=512)\n\n        self.classify1 = Sequential([\n            AveragePooling2D(pool_size=(5, 5), strides=3, padding='valid'),\n            Conv2D(160, kernel_size=(1, 1), strides=1, padding='same', activation='relu'),\n            Flatten(),\n            Dense(1024, activation='relu'),\n            Dropout(0.7),\n            Dense(512, activation='relu'),\n            Dropout(0.7),\n            Dense(1, activation='sigmoid')\n        ])\n        self.seq2 = Sequential([\n            Inception(160, 112, 80),\n            Inception(128, 128, 128),\n            Inception(112, 144, 160)\n        ])\n        self.classify2 = Sequential([\n            AveragePooling2D(pool_size=(5, 5), strides=3, padding='valid'),\n            Conv2D(112, kernel_size=(1, 1), strides=1, padding='same', activation='relu'),\n            Flatten(),\n            Dense(1024, activation='relu'),\n            Dropout(0.7),\n            Dense(512, activation='relu'),\n            Dropout(0.7),\n            Dense(1, activation='sigmoid')\n        ])\n        self.seq3 = Sequential([\n            Inception(256, 160, 160),\n            MaxPooling2D(pool_size=(3, 3), strides=2, padding='same'),\n            Inception(256, 160, 160),\n            Inception(384, 192, 64),\n            AveragePooling2D(pool_size=(7, 7), strides=1, padding='valid'),\n            Flatten(),\n            Dropout(0.4),\n            Dense(512, activation='relu'),\n            Dropout(0.4),\n            Dense(256, activation='relu'),\n            Dropout(0.4),\n            Dense(1, activation='sigmoid')\n        ])\n\n    def call(self, inputs):\n        x = self.seq1(inputs)\n        x = self.transformer(x, training=True)\n        classifier1 = tf.squeeze(self.classify1(x))\n        x = self.seq2(x)\n        classifier2 = tf.squeeze(self.classify2(x))\n        final_classifier = tf.squeeze(self.seq3(x))\n        return classifier1, classifier2, final_classifier\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"initial_weights = model.get_weights()","metadata":{"execution":{"iopub.status.busy":"2024-06-27T10:55:36.622700Z","iopub.execute_input":"2024-06-27T10:55:36.623075Z","iopub.status.idle":"2024-06-27T10:55:36.726202Z","shell.execute_reply.started":"2024-06-27T10:55:36.623046Z","shell.execute_reply":"2024-06-27T10:55:36.725435Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"initial_weights[0].shape","metadata":{"execution":{"iopub.status.busy":"2024-06-27T10:56:19.957709Z","iopub.execute_input":"2024-06-27T10:56:19.958626Z","iopub.status.idle":"2024-06-27T10:56:19.964328Z","shell.execute_reply.started":"2024-06-27T10:56:19.958590Z","shell.execute_reply":"2024-06-27T10:56:19.963311Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"(7, 7, 3, 64)"},"metadata":{}}]},{"cell_type":"code","source":"model=Network()\noptimizer=Adam(learning_rate=0.001)\nloss_fn=BinaryCrossentropy()\n@tf.function\ndef train_step(images, labels):\n    with tf.GradientTape() as tape:\n        classifier1,classifier2,final_classifier= model(images)\n        classifier1_loss = loss_fn(labels, classifier1)\n        classifier2_loss = loss_fn(labels, classifier2)\n        final_classifier_loss = loss_fn(labels, final_classifier)\n        final_loss = classifier1_loss + classifier2_loss + final_classifier_loss\n    gradients = tape.gradient(final_loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    return final_loss,final_classifier\n","metadata":{"execution":{"iopub.status.busy":"2024-06-27T12:20:08.135284Z","iopub.execute_input":"2024-06-27T12:20:08.135674Z","iopub.status.idle":"2024-06-27T12:20:08.298824Z","shell.execute_reply.started":"2024-06-27T12:20:08.135643Z","shell.execute_reply":"2024-06-27T12:20:08.297806Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def calculate_accuracy(labels, predictions):\n    predictions = tf.sigmoid(predictions)  \n    predicted_classes = tf.round(predictions)  \n    labels = tf.squeeze(tf.cast(labels, tf.int32))  \n    predicted_classes = tf.squeeze(tf.cast(predicted_classes, tf.int32))  \n    accuracy = tf.reduce_mean(tf.cast(tf.equal(labels, predicted_classes), tf.float32))\n    return accuracy\n\ndef calculate_precision(labels, predictions):\n    predictions = tf.sigmoid(predictions)  \n    predicted_classes = tf.round(predictions)  \n    labels = tf.squeeze(tf.cast(labels, tf.int32))  \n    predicted_classes = tf.squeeze(tf.cast(predicted_classes, tf.int32))  \n    \n    true_positives = tf.reduce_sum(tf.cast(predicted_classes * labels, tf.float32))\n    predicted_positives = tf.reduce_sum(tf.cast(predicted_classes, tf.float32))\n    \n    precision = true_positives / (predicted_positives + tf.keras.backend.epsilon())\n    return precision\n","metadata":{"execution":{"iopub.status.busy":"2024-06-27T12:20:14.170841Z","iopub.execute_input":"2024-06-27T12:20:14.171238Z","iopub.status.idle":"2024-06-27T12:20:14.181107Z","shell.execute_reply.started":"2024-06-27T12:20:14.171209Z","shell.execute_reply":"2024-06-27T12:20:14.179966Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"epochs = 10\nfor epoch in range(epochs):\n    total_loss = 0.0\n    total_accuracy = 0.0\n    total_precision = 0.0\n    num_batches = 0\n    initial_weights = model.get_weights()\n    for images, labels in train_dataset:\n        loss, predictions = train_step(images, labels)\n        batch_accuracy = calculate_accuracy(labels, predictions)\n        batch_precision = calculate_precision(labels, predictions)\n\n        total_loss += loss\n        total_accuracy += batch_accuracy\n        total_precision += batch_precision\n        num_batches += 1\n        \n    \n    average_loss = total_loss / num_batches\n    average_accuracy = total_accuracy / num_batches\n    average_precision = total_precision / num_batches\n    updated_weights = model.get_weights()\n    total_count=0\n    count=0\n    for initial, updated in zip(initial_weights, updated_weights):\n        total_count+=1\n        if not np.array_equal(initial, updated):\n            count+=1\n    print(f'Total Count : {total_count} ; Count :{count}')\n    print(f\"Epoch {epoch + 1}, Loss: {average_loss.numpy()}, Accuracy: {average_accuracy.numpy()}, Precision: {average_precision.numpy()}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-27T12:35:50.093458Z","iopub.execute_input":"2024-06-27T12:35:50.094373Z","iopub.status.idle":"2024-06-27T12:47:29.202589Z","shell.execute_reply.started":"2024-06-27T12:35:50.094338Z","shell.execute_reply":"2024-06-27T12:47:29.201612Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Total Count : 144 ; Count :129\nEpoch 1, Loss: 2.0795719623565674, Accuracy: 0.5, Precision: 0.5\nTotal Count : 144 ; Count :126\nEpoch 2, Loss: 2.0795915126800537, Accuracy: 0.5, Precision: 0.5\nTotal Count : 144 ; Count :140\nEpoch 3, Loss: 2.082878828048706, Accuracy: 0.5, Precision: 0.5\nTotal Count : 144 ; Count :115\nEpoch 4, Loss: 2.079641342163086, Accuracy: 0.5, Precision: 0.5\nTotal Count : 144 ; Count :114\nEpoch 5, Loss: 2.0796010494232178, Accuracy: 0.5, Precision: 0.5\nTotal Count : 144 ; Count :110\nEpoch 6, Loss: 2.0795884132385254, Accuracy: 0.5, Precision: 0.5\nTotal Count : 144 ; Count :109\nEpoch 7, Loss: 2.079555034637451, Accuracy: 0.5, Precision: 0.5\nTotal Count : 144 ; Count :112\nEpoch 8, Loss: 2.0795764923095703, Accuracy: 0.5, Precision: 0.5\nTotal Count : 144 ; Count :116\nEpoch 9, Loss: 2.0795483589172363, Accuracy: 0.5, Precision: 0.5\nTotal Count : 144 ; Count :117\nEpoch 10, Loss: 2.0795915126800537, Accuracy: 0.5, Precision: 0.5\n","output_type":"stream"}]},{"cell_type":"code","source":"initial_weights = model.get_weights()","metadata":{"execution":{"iopub.status.busy":"2024-06-27T10:55:23.653633Z","iopub.execute_input":"2024-06-27T10:55:23.654534Z","iopub.status.idle":"2024-06-27T10:55:23.783411Z","shell.execute_reply.started":"2024-06-27T10:55:23.654500Z","shell.execute_reply":"2024-06-27T10:55:23.782387Z"},"trusted":true},"execution_count":32,"outputs":[]}]}